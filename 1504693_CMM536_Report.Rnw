\documentclass[10pt  ,usenames, dvipsnames]{article}
\usepackage{graphicx, verbatim}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{lipsum}
\usepackage{todonotes}
\usepackage[tableposition=top]{caption}
\usepackage{ifthen}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{listings}
\usepackage{color}
\setlength{\textwidth}{6.5in} 
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in} 
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-1.5cm}
\setlength{\parindent}{0cm}
\usepackage{setspace}
\usepackage{float}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{tabularx}
\usepackage{lmodern} % for bold teletype font
\usepackage{minted}
\usepackage{underscore}

\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = red %Colour of citations
}

%\fancyhf{}
\rfoot{Andrew Tait \thepage}
\singlespacing
\usepackage[affil-it]{authblk} 
\usepackage{etoolbox}
\usepackage{lmodern}


% Notice the following package, it will help you cite papers
\usepackage[backend=bibtex ,sorting=none]{biblatex}
\bibliography{references}

\begin{filecontents*}{references.bib}

\end{filecontents*}


\begin{document}


\title{\LARGE Coursework  \\ Advanced Data Science (CMM536)}

\author{Andrew Tait, \textit{\href{1504693@rgu.ac.uk}{1504693@rgu.ac.uk}}}
\maketitle
% \begin{flushleft} \today \end{flushleft} 
\noindent\rule{16cm}{0.4pt}
%\underline{\hspace{3cm}
\ \\
%\thispagestyle{empty}

\section{Research}

[your text goes here]
The paper that was chosen for this work is \cite{6779381}. The authors provided a full review on different streaming algorithms and methos that handle concept drfit?. Below is a review of this paper that includes problem statement, related work and methods applied.\\

\textcolor{blue}{\textit{Notice how I cited the paper, and how does it appear in the document, to do so, you need to have a file in your working director (references.bib), this file simply contains the bibtext items for the papers you chose. These BibTex items are often avaiable to download from publishers website, see Figure~\ref{fig1}}}
\subsection{Problem Statement}
What is this paper about? your text goes here, your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here

\subsection{Relevant Work}
[your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here]
\subsection{Methods}
[your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here]
\subsection{Results}
[your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here]
\subsection{Conclusion}

[your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here your text goes here]




\section {Data Streams}

\subsection{Dataset Choice}

The dataset that has been chosen for this part of the coursework is the 'adult' dataset. This is avaiable on the UCI repository.

\url{https://archive.ics.uci.edu/ml/datasets/adult}

The dataset that has been chosen for this part of the course work is IRIS. This is available on the UCI repository. The set was chosen because of .... It proves to be a good set for evaluating 'x' methods .... 

\subsection{Data Exploration}

The main purpose of the adult dataset is to find out which characteristics of the us population affect if their income is either <=$50k or >=$50k



To start off I will clear the RStudio environment and import the required libaries.

<<eval=FALSE,comment=TRUE>>=
#Clean RStudio Environment
rm(list = ls())

#Import librarys
library(caret)
library(partykit)
library(mlbench)
library(RWeka)
library(C50)
library(datasets)
library(rpart)
library(ggplot2)
library(jsonlite)
library(data.table)
library(RMOA)
library(ROCR)
library(stream)
library(mlbench)
library(doParallel)
require(tm)
require(wordcloud)
library(wordcloud,quietly=TRUE)
library(RColorBrewer,quietly=TRUE)
@

<<warning=FALSE,echo=FALSE, comment=FALSE, message=FALSE>>==

#Import librarys
library(caret)
library(partykit)
library(mlbench)
library(RWeka)
library(C50)
library(datasets)
library(rpart)
library(ggplot2)
library(jsonlite)
library(data.table)
library(RMOA)
library(ROCR)
library(stream)
library(mlbench)
library(doParallel)
require(tm)
require(wordcloud)
library(wordcloud,quietly=TRUE)
library(RColorBrewer,quietly=TRUE)
@

Then set the working directory to the coursework folder

<<eval=FALSE,comment=TRUE>>=
#Set working directory
setwd("~/CMM536 Advanced Data Science/Coursework/CMM536_Coursework")
@

<<warning=FALSE,echo=FALSE, comment=FALSE, message=FALSE>>==
setwd("~/CMM536 Advanced Data Science/Coursework/CMM536_Coursework")
@

\clearpage

In order to the import the adult dataset, the feature names first need to be defined.
<<eval=FALSE,comment=TRUE>>=
#Feature names
adultNames <- c("age", "workclass", "fblwgt",
                "education", "education-num", 
                "martial-status",
                "occupation",
                "relationship",
                "race",
                "sex",
                "captial-gain",
                "captain-loss",
                "hours-per-week",
                "native-country",
                "class")
@

<<warning=FALSE,echo=FALSE>>==
#Feature names
adultNames <- c("age", "workclass", "fblwgt",
                "education", "education-num", 
                "martial-status",
                "occupation",
                "relationship",
                "race",
                "sex",
                "captial-gain",
                "captain-loss",
                "hours-per-week",
                "native-country",
                "class")
@

The adult dataset is then imported from adult.data file:
<<eval=FALSE,comment=TRUE>>=


#Import datasets
adult <- read.table("data/adult.data" ,header = FALSE, sep = ",",
                         strip.white = TRUE, col.names = adultNames,
                         na.strings = "?", stringsAsFactors = TRUE)
@


<<warning=FALSE,echo=FALSE>>==

#Import datasets
adult <- read.table("data/adult.data" ,header = FALSE, sep = ",",
                         strip.white = TRUE, col.names = adultNames,
                         na.strings = "?", stringsAsFactors = TRUE)
@

Now the adult dataset is imported, it's time for some basic exploration

Number of rows (instances) in the dataset

<<eval=FALSE,comment=TRUE>>=
nrow(adult)
@

<<warning=FALSE,echo=FALSE>>==
nrow(adult)
@

The number of columns (features)

<<eval=FALSE,comment=TRUE>>=
ncol(adult)
@

<<warning=FALSE,echo=FALSE>>==
ncol(adult)
@

Summary of the adult dataset:

<<eval=FALSE,comment=TRUE>>=
#inspect dataset
str(adult)
@

<<warning=FALSE,echo=FALSE>>==
str(adult)
@



Now that some basic data exploration is covered, next to inspect the dataset a bit further. Starting with the class distribution in the adult dataset, see (Figure~\ref{fig1})

<<warning=FALSE,eval=FALSE,comment=TRUE>>==
#Class Distribution
barplot(table(adult$class))
@

\begin{figure}[H]
\begin{center}
<<warning=FALSE,echo=FALSE,fig.height=8.5,out.width=".76\\linewidth">>=
#Class Distribution
barplot(table(adult$class))
@
\caption {Barplot of Class Distribution}
\label{fig1}
\end {center}
\end {figure}


\clearpage

\subsection{Build Classifier}

\subsubsection{Pre-Processing}
Before the adult dataset can classified, some pre-processing is required first.

The dataset is checked for missing values ('?')
<<warning=FALSE,eval=FALSE,comment=TRUE>>==
#Check for missing values ('?')
table(complete.cases (adult))
@

<<warning=FALSE,echo=FALSE>>==
table(complete.cases (adult))
@

As shown above there are missing values in the workclass,occupation and class columns. so these are removed.

<<warning=FALSE,eval=FALSE,comment=TRUE>>==
cleanadult = adult[!is.na(adult$workclass)& !is.na(adult$occupation) & !is.na(adult$class),]
@


<<warning=FALSE,echo=FALSE>>==
cleanadult = adult[!is.na(adult$workclass)& !is.na(adult$occupation) & !is.na(adult$class),]
@

The flwgt feature is also removed as it's not required.

<<warning=FALSE,eval=FALSE,comment=TRUE>>==
#Remove flwgt feature
cleanadult$fblwgt = NULL
@

<<warning=FALSE,echo=FALSE>>==
#Remove flwgt feature
cleanadult$fblwgt = NULL
@

Lets inspect the cleanadult dataframe before going further:

<<warning=FALSE,eval=FALSE,comment=TRUE>>==
str(cleanadult)
@

<<warning=FALSE,echo=FALSE>>==
str(cleanadult)
@


\clearpage

As the adult dataset is a mixture of factors, integers and characters, I decided that transforming the dataset into binary. This will be create features based on every possible value in the dataset.

The cleanadult dataframe is first copied and the class is removed (it's not being transforme dinto binary)

<<warning=FALSE,eval=FALSE,comment=TRUE>>==
#Copy dataset
noClass <-cleanadult
#Remove class as it is not being transformed to binary
noClass$class <- NULL
@


<<warning=FALSE,echo=FALSE>>==
#Copy dataset
noClass <-cleanadult
#Remove class as it is not being transformed to binary
noClass$class <- NULL
@

Then the noClass dataframe is transformed into binary

<<warning=FALSE,eval=FALSE,comment=TRUE>>==
binaryVars <- caret::dummyVars(~ ., data = noClass)
newAdult <- predict(binaryVars, newdata = noClass)
@

<<warning=FALSE,echo=FALSE>>==
binaryVars <- caret::dummyVars(~ ., data = noClass)
newAdult <- predict(binaryVars, newdata = noClass)
@

The class feature is then added to the binarised dataset
<<warning=FALSE,eval=FALSE,comment=TRUE>>==
#add class to binarised dataset
binAdult <-cbind(newAdult, cleanadult[14])
@

<<warning=FALSE,echo=FALSE>>==
binAdult <-cbind(newAdult, cleanadult[14])
@

Any rows with NA values after being binary transformed.
<<warning=FALSE,eval=FALSE,comment=TRUE>>==
#remove any rows with NA values
row.has.na <- apply(binAdult, 1, function(x){any(is.na(x))})
sum(row.has.na)
binAdult <- binAdult[!row.has.na,]
@

Number of NA rows removed.
<<warning=FALSE,echo=FALSE, cache=TRUE>>==
#remove any rows with NA values
row.has.na <- apply(binAdult, 1, function(x){any(is.na(x))})
sum(row.has.na)
binAdult <- binAdult[!row.has.na,]
@

\clearpage

\subsubsection{Classification}

When it came to dividing the mushroom dataset into training and testing subsets, I decided to go with 80
percent training and 20 percent testing split as a starting point/baseline. 
Since the class distrubtion is unbalenced, I thought this split would cover the majority of cases.

<<warning=FALSE,eval=FALSE,comment=TRUE>>==
#split 80% training and 20% testing datasets
inTrain <- createDataPartition(y=binAdult$class, p=0.8, list=FALSE)

#Assign indexes to split the binAdult dataset into training and testing
training <- binAdult[inTrain,]
testing <- binAdult[inTrain,]
@

<<warning=FALSE,echo=FALSE, cache=TRUE>>==
#split 80% training and 20% testing datasets
inTrain <- createDataPartition(y=binAdult$class, p=0.8, list=FALSE)

#Assign indexes to split the binAdult dataset into training and testing
training <- binAdult[inTrain,]
testing <- binAdult[inTrain,]
@


For the initial classifier I decided to go with the kNN Classifier as it has proven to be a good baseline in previous labs and exercises in R.
Before the classification begins, parallel processing is enabled to speed up this process.

<<warning=FALSE,eval=FALSE,comment=TRUE>>==
#Setup Parallel processing to speed up classification modelling
cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)
@

<<warning=FALSE,echo=FALSE>>==
#Setup Parallel processing to speed up classification modelling
cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)
@

The train control is set to repeated-cross-validation with 10 folds and 3 repeats
<<warning=FALSE,eval=FALSE,comment=TRUE>>==
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 3,
                     number = 10,
                     verboseIter=TRUE)
@


<<warning=FALSE,echo=FALSE>>==
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 3,
                     number = 10,
                     verboseIter=TRUE)
@
Next the seed is set to 1, in order to make the model reproducible and the kNN model is set up with the train control from above and k value set to 3.

<<warning=FALSE,eval=FALSE,comment=TRUE>>==
# ensure reproducibility of results by setting the seed to a known value
set.seed(1)
#use knn
mod21.knn<- train(class~., data=training, 
                  method="knn", tuneGrid=expand.grid(.k=3),trControl=ctrl)
@

<<warning=FALSE,echo=FALSE, message=FALSE, cache=TRUE>>==
set.seed(1)
#use knn
mod21.knn<- train(class~., data=training, 
                  method="knn", tuneGrid=expand.grid(.k=3),trControl=ctrl)
@


\clearpage

Once the knn Model is complete, it’s time to analyse the results, first with a print of the kNNModel as shown below.
<<warning=FALSE,eval=FALSE,comment=TRUE>>==
print(mod21.knn)
@

<<warning=FALSE,echo=FALSE>>==
print(mod21.knn)
@

\clearpage

\subsubsection {Evalulation}

To evaluate the module, a confusion matrix is produced by predicting the against the testing (20 percent of the total dataset ) subset.

<<warning=FALSE,eval=FALSE,comment=TRUE>>==
#Evaluation
predictkNN <- predict(mod21.knn,testing)
confusionMatrix(predictkNN, testing$class)
@


<<warning=FALSE,echo=FALSE, cache=TRUE>>==
#Evaluation
predictkNN <- predict(mod21.knn,testing)
confusionMatrix(predictkNN, testing$class)
@

\clearpage

\subsection{Build Stream Classifier}

Same as above. Complete this part as required by the coursework sheet. Again, be clear, visuals always helps in communicating results. Justify your choices and explain your methods. 


\clearpage

\section {Text Classification}
For this task a csv of leave/remain tweets of the brexit campaign was provided.

The was first imported:
<<warning=FALSE,eval=FALSE,comment=TRUE>>==
leaveRemainTweets <- read.csv("data/leaveRemainTweets_CW.csv", header=TRUE)
@

\subsection{Preprocessing}

In order to pre-process the leaveRemain tweets, I built a custom corpus function. Which can be found at (Figure~\ref{figbuildCorpus})


\clearpage

\subsection{Text Analysis}



\clearpage

\subsection{Text Classification}

Use one of the ‘R’ packages to build a classifier that classifies the tweets as leave tweet or remain tweets. 

Complete this part as required by the coursework sheet. Again, be clear, visuals always helps in communicating results. Justify your choices and explain your methods. 


\clearpage

\section{Appendix}

\subsection{Custom buildCorpus Function}
For the pre-processing in the text-classification task, I created a custom corpus function that will build a corpus of the leave/remain tweets and change the word filtering based on the optional function parameters.

\lstset{ 
  language=R,                     % the language of the code
  basicstyle=\tiny\ttfamily, % the size of the fonts that are used for the code
  stepnumber=1,                   % the step between two line-numbers. If it is 1, each line
                                  % will be numbered
  numbersep=5pt,                  % how far the line-numbers are from the code
  backgroundcolor=\color{white},  % choose the background color. You must add \usepackage{color}
  showspaces=false,               % show spaces adding particular underscores
  showstringspaces=false,         % underline spaces within strings
  showtabs=false,                 % show tabs within strings adding particular underscores
  frame=single,                   % adds a frame around the code
  rulecolor=\color{black},        % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. commens (green here))
  tabsize=2,                      % sets default tabsize to 2 spaces
  captionpos=b,                   % sets the caption-position to bottom
  breaklines=true,                % sets automatic line breaking
  breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
 keywordstyle=\color{RoyalBlue},      % keyword style
  commentstyle=\color{YellowGreen},   % comment style
  stringstyle=\color{ForestGreen}      % string literal style
} 
% 
% \label{figBuildCorpus}
% \begin{figure}[H]
% \caption {Custom buildCorpus function}
% \end {figure}
%     \begin{lstlisting}
%     #Custom Corpus Function for preprocessing  
% 
% buildCorpus <- function (tweets, wholeDataSet, wordAssocation){
%   corp <- Corpus(VectorSource(tweets$text))
%   
%   
%   corp <- tm_map(corp,
%                  content_transformer(function(x) iconv(x, to='ASCII',
%                                                        sub='byte')))
%   # remove stop words and other preprocessing
%  
%   corp <- tm_map(corp, content_transformer(tolower))
%   corp <- tm_map(corp, removeNumbers)
% 
%   toSpace = content_transformer( function(x, pattern) gsub(pattern," ",x) )
%   
%   
%   ##Tweet cleaning
%   
%   #credit to https://stackoverflow.com/a/31352005/8816204
%   corp <-tm_map(corp, toSpace, "(RT|via)((?:\\b\\W*@\\w+)+)")
%   corp <-tm_map(corp, toSpace, "@\\w+")
%   corp <-tm_map(corp, toSpace, "&amp")
%   
%   ##Remove URLS from tweets
%   corp <-tm_map(corp, toSpace, "httpsw+")
%   corp <-tm_map(corp, toSpace, "http:\\w+")
%   corp <-tm_map(corp, toSpace, "https:\\w+")
% 
%   corp <-tm_map(corp, toSpace, "[ \t]{2,}")
%   corp <-tm_map(corp, toSpace, "^\\s+|\\s+$")
%   
% 
%   # ##Remove obvious words /stopwords
%   if(missing(wholeDataSet)){
%     corp <- tm_map(corp, function(x)removeWords(x,c(stopwords("english"),"amp", "will", "‰Û_", "https", "http", "httpsdb")))
%   } else if(wholeDataSet == TRUE){
%     corp <- tm_map(corp, function(x)removeWords(x,c(stopwords("english"),"amp", "will", "‰Û_", "https", "http", "httpsdb", "eu", "brexit", "rt", "leave", "remain", "vote")))
%   }
% 
%   #remove punctuation last so urls are removed correctly
%   corp <- tm_map(corp, removePunctuation)
%   
%   if(missing(wordAssocation)){
%     tdm <- TermDocumentMatrix(corp)
%     
%     m <- as.matrix(tdm)
%     v <- sort(rowSums(m), decreasing = TRUE)
%     d <- data.frame(word = names(v), freq = v)
%     d$word <- gsub("˜", " ", d$word) ## Edit 2
%     
%     tweets <- d$word
%   }else if(wordAssocation == TRUE){
%     dtm <- DocumentTermMatrix(corp)
%   }
% }
% \end{lstlisting}

% Clear the page and starte a new page for references 

\clearpage
% The title for the reference section is called References 

\section{References}\label{pubs}

\printbibliography[heading =none]


\clearpage


\end{document}
